{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Non-Inferiority & Performance Analysis: AI vs. Human\n",
    "\n",
    "This notebook computes a non-inferiority test for multiple AI classifiers (AI1-AI4) against a single Human classifier (H1) using a **paired, stratified bootstrap** on both the **Macro-Averaged Weighted F1-score (Macro W-F1)** and **Macro-Averaged Unweighted F1-score (Macro F1)**.\n",
    "\n",
    "The 'Macro' calculation means performance is computed per disease first, and then those scores are averaged.\n",
    "\n",
    "It also includes:\n",
    "1.  **Non-Inferiority Tests:** Both weighted (by `log(1 + concept_count)`) and unweighted.\n",
    "2.  **Performance Metrics:** Weighted and unweighted F-score, precision, and recall, calculated as a macro-average (Overall) and per disease.\n",
    "3.  **Agreement Plots:** Stacked bar charts visualizing True Positives, False Positives, and False Negatives for all classifiers, overall and per disease.\n",
    "\n",
    "### 1. Configuration\n",
    "Set the paths and parameters for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# âš™ï¸ CONFIGURATION\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- Statistical Parameters ---\n",
    "# âš ï¸ Define your non-inferiority margin (Î”).\n",
    "NON_INFERIORITY_MARGIN = 0.05\n",
    "BOOTSTRAP_ITERATIONS = 10000\n",
    "ALPHA = 0.05  # For a 95% one-sided CI (or 90% two-sided CI)\n",
    "\n",
    "# --- Classifier IDs ---\n",
    "HUMAN_ARM_ID = \"H1\"\n",
    "AI_ARM_IDS = [\"AI1\", \"AI2\", \"AI3\", \"AI4\"] # Test all AI arms\n",
    "\n",
    "# --- Path Configuration ---\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()  # Fallback for interactive notebooks\n",
    "\n",
    "GOLD_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold_final\")\n",
    "CONCEPTSET_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, \"..\", \"ConceptSets\"))\n",
    "RECORD_COUNT_PATH = os.path.join(SCRIPT_DIR, \"ConceptRecordCounts.csv\")\n",
    "\n",
    "print(f\"Notebook directory: {SCRIPT_DIR}\")\n",
    "print(f\"Gold Standard directory: {GOLD_DIR}\")\n",
    "print(f\"ConceptSet directory: {CONCEPTSET_DIR}\")\n",
    "print(f\"Record Count file: {RECORD_COUNT_PATH}\")\n",
    "print(f\"---\")\n",
    "print(f\"Comparing AI arms: {', '.join(AI_ARM_IDS)}\")\n",
    "print(f\"Against Human arm: {HUMAN_ARM_ID}\")\n",
    "print(f\"Non-Inferiority Margin (Î”): {NON_INFERIORITY_MARGIN}\")\n",
    "print(f\"Test: F1(AI) - F1(Human) > -{NON_INFERIORITY_MARGIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions\n",
    "These functions find and load the concept files for each task and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concept_file(disease_prefix, arm_id, concept_dir):\n",
    "    \"\"\"Finds the includedConcepts.csv file for a specific disease prefix and arm ID.\"\"\"\n",
    "    pattern = re.compile(rf\"\\[{disease_prefix}\\](.*?)\\[{arm_id}\\]\", re.IGNORECASE)\n",
    "    online_pattern = re.compile(r\"ONLINE\", re.IGNORECASE)\n",
    "    \n",
    "    for folder_name in os.listdir(concept_dir):\n",
    "        folder_path = os.path.join(concept_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "            \n",
    "        if pattern.search(folder_name) and not online_pattern.search(folder_name):\n",
    "            csv_path = os.path.join(folder_path, \"includedConcepts.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                return csv_path\n",
    "    return None\n",
    "\n",
    "def load_concepts(file_path):\n",
    "    \"\"\"Loads concept IDs from a CSV file into a set.\"\"\"\n",
    "    if file_path is None or not os.path.exists(file_path):\n",
    "        print(f\"âš ï¸ File not found, returning empty set: {file_path}\")\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        \n",
    "        target_col = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() == \"conceptid\":\n",
    "                target_col = col\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "             for col in df.columns:\n",
    "                 if \"concept\" in col.lower() and \"id\" in col.lower():\n",
    "                    target_col = col\n",
    "                    break\n",
    "        \n",
    "        if target_col is None:\n",
    "            print(f\"âš ï¸ Could not find 'conceptId' in {file_path}. Using first column.\")\n",
    "            target_col = df.columns[0]\n",
    "            \n",
    "        vals = df[target_col].dropna().astype(str).str.strip().str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "        return set(vals[vals != \"\"])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {file_path}: {e}\")\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Concept Weights\n",
    "Load the `ConceptRecordCounts.csv` file and create the `log(1 + count)` weight for each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RECORD_COUNT_PATH):\n",
    "    print(f\"âŒ CRITICAL ERROR: ConceptRecordCounts.csv not found at {RECORD_COUNT_PATH}\")\n",
    "    print(\"Please ensure the file is in the correct location.\")\n",
    "    df_counts = pd.DataFrame(columns=[\"concept_id\", \"record_count\"])\n",
    "else:\n",
    "    df_counts = pd.read_csv(RECORD_COUNT_PATH, dtype=str)\n",
    "    print(f\"Loaded {len(df_counts)} records from {RECORD_COUNT_PATH}\")\n",
    "\n",
    "# Ensure correct dtypes\n",
    "df_counts['concept_id'] = df_counts['concept_id'].astype(str).str.strip().str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "df_counts['record_count'] = pd.to_numeric(df_counts['record_count'], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate the log(1+count) weight\n",
    "df_counts['weight'] = np.log1p(df_counts['record_count'])\n",
    "\n",
    "# Create a lookup dictionary (Series) for fast mapping\n",
    "weight_lookup = df_counts.set_index('concept_id')['weight']\n",
    "\n",
    "print(f\"Created weight lookup for {len(weight_lookup)} concepts.\")\n",
    "display(weight_lookup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build Master Item-Level Dataset\n",
    "\n",
    "We now loop through all tasks (diseases) and build one large DataFrame. Each row represents a single concept (item) and its classification by all parties, plus its concept weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = []\n",
    "all_tasks_found = []\n",
    "classifier_columns = ['human'] + [arm.lower() for arm in AI_ARM_IDS]\n",
    "\n",
    "gold_files = glob.glob(os.path.join(GOLD_DIR, \"*_Gold_Standard_FINAL.csv\"))\n",
    "print(f\"Found {len(gold_files)} Gold Standard files.\")\n",
    "\n",
    "for gs_file in gold_files:\n",
    "    base_name = os.path.basename(gs_file)\n",
    "    match = re.search(r\"^(C\\d+)\", base_name, re.IGNORECASE)\n",
    "    if not match:\n",
    "        print(f\"Skipping {base_name}: Could not parse disease prefix.\")\n",
    "        continue\n",
    "        \n",
    "    disease_prefix = match.group(1)\n",
    "    task_name = base_name.replace(\"_Gold_Standard_FINAL.csv\", \"\")\n",
    "    \n",
    "    # Find corresponding classifier files\n",
    "    human_file = find_concept_file(disease_prefix, HUMAN_ARM_ID, CONCEPTSET_DIR)\n",
    "    ai_files = {arm: find_concept_file(disease_prefix, arm, CONCEPTSET_DIR) for arm in AI_ARM_IDS}\n",
    "    \n",
    "    all_files_found = human_file is not None and all(ai_files.values())\n",
    "    if not all_files_found:\n",
    "        print(f\"\\nâš ï¸ Skipping {task_name} (prefix {disease_prefix}):\")\n",
    "        if not human_file: \n",
    "            print(f\"  - Missing file for {HUMAN_ARM_ID}\")\n",
    "        for arm, f in ai_files.items():\n",
    "            if f is None:\n",
    "                print(f\"  - Missing file for {arm}\")\n",
    "        continue\n",
    "        \n",
    "    # Load concept sets\n",
    "    gs_concepts = load_concepts(gs_file)\n",
    "    human_concepts = load_concepts(human_file)\n",
    "    ai_concepts = {arm: load_concepts(f) for arm, f in ai_files.items()}\n",
    "    \n",
    "    # Define the 'universe' of items for this task\n",
    "    universe = gs_concepts | human_concepts\n",
    "    for arm_concepts in ai_concepts.values():\n",
    "        universe.update(arm_concepts)\n",
    "    \n",
    "    if not universe:\n",
    "        print(f\"Skipping {task_name}: No concepts found.\")\n",
    "        continue\n",
    "\n",
    "    # Create item-level records\n",
    "    for concept_id in universe:\n",
    "        record = {\n",
    "            'task_id': task_name,\n",
    "            'concept_id': concept_id,\n",
    "            'gold': concept_id in gs_concepts,\n",
    "            'human': concept_id in human_concepts,\n",
    "            'weight': weight_lookup.get(concept_id, 0.0) # Get weight, default to 0.0 (log(1+0))\n",
    "        }\n",
    "        # Add AI classifications\n",
    "        for arm in AI_ARM_IDS:\n",
    "            record[arm.lower()] = concept_id in ai_concepts[arm]\n",
    "        \n",
    "        all_items.append(record)\n",
    "    \n",
    "    print(f\"Processed {task_name}: {len(universe)} items.\")\n",
    "    all_tasks_found.append(task_name)\n",
    "\n",
    "# Create the final DataFrame\n",
    "df_items = pd.DataFrame(all_items)\n",
    "\n",
    "df_items.to_csv(\"Master_data.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Master dataset created.\")\n",
    "print(f\"Total tasks included: {len(all_tasks_found)}\")\n",
    "print(f\"Total items (concepts): {len(df_items)}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# âš ï¸ BUG FIX: Check if df_items is empty before proceeding\n",
    "# ----------------------------------------------------------------\n",
    "if df_items.empty:\n",
    "    print(\"\\nâŒ CRITICAL ERROR: No data was loaded into the master DataFrame.\")\n",
    "    print(\"This happens if no tasks were processed (e.g., missing Gold Standard or arm files).\")\n",
    "    print(\"Please check the log output above. Halting execution.\")\n",
    "    # This will cause a NameError if you try to run subsequent cells, which is intended.\n",
    "    raise SystemExit(\"Stopping notebook due to empty master DataFrame.\")\n",
    "else:\n",
    "    display(df_items.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define F1-Score Calculation Functions\n",
    "\n",
    "These functions compute the **macro-averaged** F1-scores for the bootstrap loops.\n",
    "We also define the `calculate_metrics` helper here, which is used by both the bootstrap loops and the final performance tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, clf, weighted=True):\n",
    "    \"\"\"Calculates precision, recall, and F1 for a single classifier on a dataframe.\n",
    "    This assumes the df is for a *single task* or is a *micro-average* calculation.\n",
    "    \"\"\"\n",
    "    is_gold = df['gold']\n",
    "    is_clf = df[clf]\n",
    "    \n",
    "    if weighted:\n",
    "        weights = df['weight']\n",
    "        # Multiply the boolean mask by the weight, then sum\n",
    "        TP = ((is_gold & is_clf) * weights).sum()\n",
    "        FP = ((~is_gold & is_clf) * weights).sum()\n",
    "        FN = ((is_gold & ~is_clf) * weights).sum()\n",
    "    else:\n",
    "        # Just sum the booleans\n",
    "        TP = (is_gold & is_clf).sum()\n",
    "        FP = (~is_gold & is_clf).sum()\n",
    "        FN = (is_gold & ~is_clf).sum()\n",
    "    \n",
    "    # Calculate metrics, handling zero denominators\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "    \n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "def calculate_macro_weighted_f1_scores(df, classifier_cols):\n",
    "    \"\"\"Calculates Macro-Averaged Weighted F1 scores for all specified classifiers.\"\"\"\n",
    "    task_scores = {clf: [] for clf in classifier_cols}\n",
    "    \n",
    "    # Group by task, calculate F1 for each task, then store it\n",
    "    for task_id, group_df in df.groupby('task_id'):\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        for clf in classifier_cols:\n",
    "            metrics = calculate_metrics(group_df, clf, weighted=True)\n",
    "            task_scores[clf].append(metrics['f1'])\n",
    "    \n",
    "    # Average the F1 scores from all tasks\n",
    "    final_scores = {}\n",
    "    for clf in classifier_cols:\n",
    "        scores = task_scores[clf]\n",
    "        final_scores[clf] = np.mean(scores) if scores else 0\n",
    "        \n",
    "    return final_scores\n",
    "\n",
    "def calculate_macro_unweighted_f1_scores(df, classifier_cols):\n",
    "    \"\"\"Calculates Macro-Averaged Unweighted F1 scores for all specified classifiers.\"\"\"\n",
    "    task_scores = {clf: [] for clf in classifier_cols}\n",
    "    \n",
    "    # Group by task, calculate F1 for each task, then store it\n",
    "    for task_id, group_df in df.groupby('task_id'):\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        for clf in classifier_cols:\n",
    "            metrics = calculate_metrics(group_df, clf, weighted=False)\n",
    "            task_scores[clf].append(metrics['f1'])\n",
    "    \n",
    "    # Average the F1 scores from all tasks\n",
    "    final_scores = {}\n",
    "    for clf in classifier_cols:\n",
    "        scores = task_scores[clf]\n",
    "        final_scores[clf] = np.mean(scores) if scores else 0\n",
    "        \n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run Stratified Bootstrap (Macro Weighted F1)\n",
    "\n",
    "This is the core of the first test. We loop 10,000 times. In each loop, we:\n",
    "1.  Create a new dataset by sampling *with replacement* from each task.\n",
    "2.  Calculate the **Macro-Averaged Weighted F1 scores** for this new dataset.\n",
    "3.  Store the difference for each AI: $MacroW-F1_{AI_n} - MacroW-F1_{Human}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original, observed Macro Weighted F1 scores and differences\n",
    "obs_scores_w = calculate_macro_weighted_f1_scores(df_items, classifier_columns)\n",
    "obs_diffs_w = {}\n",
    "\n",
    "print(\"--- Observed Macro Weighted F1 Scores ---\")\n",
    "print(f\"Observed Macro W-F1 ({HUMAN_ARM_ID}): {obs_scores_w['human']:.4f}\")\n",
    "for arm in AI_ARM_IDS:\n",
    "    arm_key = arm.lower()\n",
    "    obs_diff = obs_scores_w[arm_key] - obs_scores_w['human']\n",
    "    obs_diffs_w[arm] = obs_diff\n",
    "    print(f\"Observed Macro W-F1 ({arm}):     {obs_scores_w[arm_key]:.4f} (Diff: {obs_diff:+.4f})\")\n",
    "\n",
    "# Initialize storage for bootstrap differences\n",
    "bootstrap_diffs_w = {arm: [] for arm in AI_ARM_IDS}\n",
    "\n",
    "tasks = df_items['task_id'].unique()\n",
    "task_indices = {task: df_items[df_items['task_id'] == task].index for task in tasks}\n",
    "\n",
    "print(f\"\\nRunning {BOOTSTRAP_ITERATIONS} bootstrap iterations for Macro Weighted F1...\")\n",
    "\n",
    "# Set a fixed random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in tqdm(range(BOOTSTRAP_ITERATIONS), desc=\"Macro W-F1 Bootstrap\"):\n",
    "    # Create stratified sample\n",
    "    resampled_indices = []\n",
    "    for task in tasks:\n",
    "        indices = task_indices[task]\n",
    "        # Sample with replacement from this task's items\n",
    "        sample = np.random.choice(indices, size=len(indices), replace=True)\n",
    "        resampled_indices.append(sample)\n",
    "    \n",
    "    # Combine indices and create the bootstrap dataset\n",
    "    df_boot = df_items.loc[np.concatenate(resampled_indices)]\n",
    "    \n",
    "    # Calculate Macro Weighted F1 scores for this sample\n",
    "    scores = calculate_macro_weighted_f1_scores(df_boot, classifier_columns)\n",
    "    f1_h = scores['human']\n",
    "    \n",
    "    # Store the difference for each AI arm\n",
    "    for arm in AI_ARM_IDS:\n",
    "        f1_ai = scores[arm.lower()]\n",
    "        bootstrap_diffs_w[arm].append(f1_ai - f1_h)\n",
    "\n",
    "print(\"Macro Weighted F1 Bootstrap complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Results: Macro Weighted F1 Non-Inferiority\n",
    "\n",
    "We now analyze the 10,000 Macro Weighted F1 differences for *each AI arm* to build their confidence intervals and make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"    FINAL NON-INFERIORITY TEST RESULTS (MACRO WEIGHTED F1)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for arm in AI_ARM_IDS:\n",
    "    arm_key = arm.lower()\n",
    "    diffs_arr = np.array(bootstrap_diffs_w[arm])\n",
    "    \n",
    "    # For a one-sided test at ALPHA=0.05, we need the 5th percentile.\n",
    "    # This corresponds to the lower bound of a 90% two-sided CI.\n",
    "    ci_lower = np.percentile(diffs_arr, 100 * ALPHA)\n",
    "    ci_upper = np.percentile(diffs_arr, 100 * (1 - ALPHA))\n",
    "    mean_diff = np.mean(diffs_arr)\n",
    "    \n",
    "    # --- The Non-Inferiority Test ---\n",
    "    # We REJECT the null (H0: AI is inferior) if the lower bound \n",
    "    # of our CI is GREATER than the negative margin.\n",
    "    is_non_inferior = ci_lower > -NON_INFERIORITY_MARGIN\n",
    "    \n",
    "    # --- Final Report for this Arm ---\n",
    "    print(f\"\\n--- [ {arm} vs. {HUMAN_ARM_ID} ] ---\")\n",
    "    print(f\"Non-Inferiority Margin (Î”): {NON_INFERIORITY_MARGIN:.4f}\")\n",
    "    print(f\"Hypothesis (H1):      Macro W-F1({arm}) - Macro W-F1({HUMAN_ARM_ID}) > -{NON_INFERIORITY_MARGIN}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Observed Macro W-F1 ({HUMAN_ARM_ID}):  {obs_scores_w['human']:.4f}\")\n",
    "    print(f\"Observed Macro W-F1 ({arm}):     {obs_scores_w[arm_key]:.4f}\")\n",
    "    print(f\"Observed Difference:  {obs_diffs_w[arm]:+.4f}\")\n",
    "    print(f\"Mean Bootstrapped Diff: {mean_diff:+.4f}\")\n",
    "    print(f\"90% Confidence Interval: [{ci_lower:+.4f}, {ci_upper:+.4f}]\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Test Condition:       Lower Bound > -Margin\")\n",
    "    print(f\"Result:               {ci_lower:+.4f} > {-NON_INFERIORITY_MARGIN}\")\n",
    "    print(\"\\n   CONCLUSION (Macro Weighted F1):\")\n",
    "    if is_non_inferior:\n",
    "        print(f\"    âœ… The {arm} classifier IS non-inferior to the {HUMAN_ARM_ID} classifier.\")\n",
    "        print(\"       (The lower bound of the 90% CI is above the non-inferiority margin.)\")\n",
    "    else:\n",
    "        print(f\"    âŒ The {arm} classifier is NOT non-inferior to the {HUMAN_ARM_ID} classifier.\")\n",
    "        print(\"       (The lower bound of the 90% CI is at or below the non-inferiority margin.)\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"    Macro Weighted F1 tests complete.\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run Stratified Bootstrap (Macro Unweighted F1)\n",
    "\n",
    "This is the second test, using the same methodology but with the **Macro-Averaged Unweighted F1-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original, observed Macro Unweighted F1 scores and differences\n",
    "obs_scores_u = calculate_macro_unweighted_f1_scores(df_items, classifier_columns)\n",
    "obs_diffs_u = {}\n",
    "\n",
    "print(\"--- Observed Macro F1 Scores ---\")\n",
    "print(f\"Observed Macro F1 ({HUMAN_ARM_ID}): {obs_scores_u['human']:.4f}\")\n",
    "for arm in AI_ARM_IDS:\n",
    "    arm_key = arm.lower()\n",
    "    obs_diff = obs_scores_u[arm_key] - obs_scores_u['human']\n",
    "    obs_diffs_u[arm] = obs_diff\n",
    "    print(f\"Observed Macro F1 ({arm}):     {obs_scores_u[arm_key]:.4f} (Diff: {obs_diff:+.4f})\")\n",
    "\n",
    "# Initialize storage for bootstrap differences\n",
    "bootstrap_diffs_u = {arm: [] for arm in AI_ARM_IDS}\n",
    "\n",
    "# tasks and task_indices are already defined from the previous bootstrap\n",
    "\n",
    "print(f\"\\nRunning {BOOTSTRAP_ITERATIONS} bootstrap iterations for Macro Unweighted F1...\")\n",
    "\n",
    "# Set a fixed random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in tqdm(range(BOOTSTRAP_ITERATIONS), desc=\"Macro F1 Bootstrap\"):\n",
    "    # Create stratified sample\n",
    "    resampled_indices = []\n",
    "    for task in tasks:\n",
    "        indices = task_indices[task]\n",
    "        # Sample with replacement from this task's items\n",
    "        sample = np.random.choice(indices, size=len(indices), replace=True)\n",
    "        resampled_indices.append(sample)\n",
    "    \n",
    "    # Combine indices and create the bootstrap dataset\n",
    "    df_boot = df_items.loc[np.concatenate(resampled_indices)]\n",
    "    \n",
    "    # Calculate Macro Unweighted F1 scores for this sample\n",
    "    scores = calculate_macro_unweighted_f1_scores(df_boot, classifier_columns)\n",
    "    f1_h = scores['human']\n",
    "    \n",
    "    # Store the difference for each AI arm\n",
    "    for arm in AI_ARM_IDS:\n",
    "        f1_ai = scores[arm.lower()]\n",
    "        bootstrap_diffs_u[arm].append(f1_ai - f1_h)\n",
    "\n",
    "print(\"Macro Unweighted F1 Bootstrap complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Results: Macro Unweighted F1 Non-Inferiority\n",
    "\n",
    "We now analyze the 10,000 Macro Unweighted F1 differences to make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"    FINAL NON-INFERIORITY TEST RESULTS (MACRO UNWEIGHTED F1)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for arm in AI_ARM_IDS:\n",
    "    arm_key = arm.lower()\n",
    "    diffs_arr = np.array(bootstrap_diffs_u[arm])\n",
    "    \n",
    "    # For a one-sided test at ALPHA=0.05, we need the 5th percentile.\n",
    "    ci_lower = np.percentile(diffs_arr, 100 * ALPHA)\n",
    "    ci_upper = np.percentile(diffs_arr, 100 * (1 - ALPHA))\n",
    "    mean_diff = np.mean(diffs_arr)\n",
    "    \n",
    "    # --- The Non-Inferiority Test ---\n",
    "    is_non_inferior = ci_lower > -NON_INFERIORITY_MARGIN\n",
    "    \n",
    "    # --- Final Report for this Arm ---\n",
    "    print(f\"\\n--- [ {arm} vs. {HUMAN_ARM_ID} ] ---\")\n",
    "    print(f\"Non-Inferiority Margin (Î”): {NON_INFERIORITY_MARGIN:.4f}\")\n",
    "    print(f\"Hypothesis (H1):      Macro F1({arm}) - Macro F1({HUMAN_ARM_ID}) > -{NON_INFERIORITY_MARGIN}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Observed Macro