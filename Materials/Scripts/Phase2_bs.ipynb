{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Non-Inferiority Test: AI vs. Human (Weighted F1 Bootstrap)\n",
    "\n",
    "This notebook computes a non-inferiority test for multiple AI classifiers (AI1-AI4) against a single Human classifier (H1) using a **paired, stratified bootstrap** on the **Weighted F1-score (W-F1)** difference.\n",
    "\n",
    "The F1-score is weighted by `log(1 + concept_count)`.\n",
    "\n",
    "### 1. Configuration\n",
    "Set the paths and parameters for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# ‚öôÔ∏è CONFIGURATION\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- Statistical Parameters ---\n",
    "# ‚ö†Ô∏è Define your non-inferiority margin (Œî).\n",
    "NON_INFERIORITY_MARGIN = 0.05\n",
    "BOOTSTRAP_ITERATIONS = 10000\n",
    "ALPHA = 0.05  # For a 95% one-sided CI (or 90% two-sided CI)\n",
    "\n",
    "# --- Classifier IDs ---\n",
    "HUMAN_ARM_ID = \"H1\"\n",
    "AI_ARM_IDS = [\"AI1\", \"AI2\", \"AI3\", \"AI4\"] # Test all AI arms\n",
    "\n",
    "# --- Path Configuration ---\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()  # Fallback for interactive notebooks\n",
    "\n",
    "GOLD_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold_final\")\n",
    "CONCEPTSET_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, \"..\", \"ConceptSets\"))\n",
    "RECORD_COUNT_PATH = os.path.join(SCRIPT_DIR, \"ConceptRecordCounts.csv\")\n",
    "\n",
    "print(f\"Notebook directory: {SCRIPT_DIR}\")\n",
    "print(f\"Gold Standard directory: {GOLD_DIR}\")\n",
    "print(f\"ConceptSet directory: {CONCEPTSET_DIR}\")\n",
    "print(f\"Record Count file: {RECORD_COUNT_PATH}\")\n",
    "print(f\"---\")\n",
    "print(f\"Comparing AI arms: {', '.join(AI_ARM_IDS)}\")\n",
    "print(f\"Against Human arm: {HUMAN_ARM_ID}\")\n",
    "print(f\"Non-Inferiority Margin (Œî): {NON_INFERIORITY_MARGIN}\")\n",
    "print(f\"Test: W-F1(AI) - W-F1(Human) > -{NON_INFERIORITY_MARGIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions\n",
    "These functions find and load the concept files for each task and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_concept_file(disease_prefix, arm_id, concept_dir):\n",
    "    \"\"\"Finds the includedConcepts.csv file for a specific disease prefix and arm ID.\"\"\"\n",
    "    pattern = re.compile(rf\"\\[{disease_prefix}\\](.*?)\\[{arm_id}\\]\", re.IGNORECASE)\n",
    "    online_pattern = re.compile(r\"ONLINE\", re.IGNORECASE)\n",
    "    \n",
    "    for folder_name in os.listdir(concept_dir):\n",
    "        folder_path = os.path.join(concept_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "            \n",
    "        if pattern.search(folder_name) and not online_pattern.search(folder_name):\n",
    "            csv_path = os.path.join(folder_path, \"includedConcepts.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                return csv_path\n",
    "    return None\n",
    "\n",
    "def load_concepts(file_path):\n",
    "    \"\"\"Loads concept IDs from a CSV file into a set.\"\"\"\n",
    "    if file_path is None or not os.path.exists(file_path):\n",
    "        print(f\"‚ö†Ô∏è File not found, returning empty set: {file_path}\")\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        \n",
    "        target_col = None\n",
    "        for col in df.columns:\n",
    "            if col.lower() == \"conceptid\":\n",
    "                target_col = col\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "             for col in df.columns:\n",
    "                 if \"concept\" in col.lower() and \"id\" in col.lower():\n",
    "                    target_col = col\n",
    "                    break\n",
    "        \n",
    "        if target_col is None:\n",
    "            print(f\"‚ö†Ô∏è Could not find 'conceptId' in {file_path}. Using first column.\")\n",
    "            target_col = df.columns[0]\n",
    "            \n",
    "        vals = df[target_col].dropna().astype(str).str.strip().str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "        return set(vals[vals != \"\"])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        return set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Concept Weights\n",
    "Load the `ConceptRecordCounts.csv` file and create the `log(1 + count)` weight for each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RECORD_COUNT_PATH):\n",
    "    print(f\"‚ùå CRITICAL ERROR: ConceptRecordCounts.csv not found at {RECORD_COUNT_PATH}\")\n",
    "    print(\"Please ensure the file is in the correct location.\")\n",
    "    df_counts = pd.DataFrame(columns=[\"concept_id\", \"record_count\"])\n",
    "else:\n",
    "    df_counts = pd.read_csv(RECORD_COUNT_PATH, dtype=str)\n",
    "    print(f\"Loaded {len(df_counts)} records from {RECORD_COUNT_PATH}\")\n",
    "\n",
    "# Ensure correct dtypes\n",
    "df_counts['concept_id'] = df_counts['concept_id'].astype(str).str.strip().str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "df_counts['record_count'] = pd.to_numeric(df_counts['record_count'], errors='coerce').fillna(0)\n",
    "\n",
    "# Calculate the log(1+count) weight\n",
    "df_counts['weight'] = np.log1p(df_counts['record_count'])\n",
    "\n",
    "# Create a lookup dictionary (Series) for fast mapping\n",
    "weight_lookup = df_counts.set_index('concept_id')['weight']\n",
    "\n",
    "print(f\"Created weight lookup for {len(weight_lookup)} concepts.\")\n",
    "display(weight_lookup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build Master Item-Level Dataset\n",
    "\n",
    "We now loop through all tasks (diseases) and build one large DataFrame. Each row represents a single concept (item) and its classification by all parties, plus its concept weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = []\n",
    "all_tasks_found = []\n",
    "classifier_columns = ['human'] + [arm.lower() for arm in AI_ARM_IDS]\n",
    "\n",
    "gold_files = glob.glob(os.path.join(GOLD_DIR, \"*_Gold_Standard_FINAL.csv\"))\n",
    "print(f\"Found {len(gold_files)} Gold Standard files.\")\n",
    "\n",
    "for gs_file in gold_files:\n",
    "    base_name = os.path.basename(gs_file)\n",
    "    match = re.search(r\"^(C\\d+)\", base_name, re.IGNORECASE)\n",
    "    if not match:\n",
    "        print(f\"Skipping {base_name}: Could not parse disease prefix.\")\n",
    "        continue\n",
    "        \n",
    "    disease_prefix = match.group(1)\n",
    "    task_name = base_name.replace(\"_Gold_Standard_FINAL.csv\", \"\")\n",
    "    \n",
    "    # Find corresponding classifier files\n",
    "    human_file = find_concept_file(disease_prefix, HUMAN_ARM_ID, CONCEPTSET_DIR)\n",
    "    ai_files = {arm: find_concept_file(disease_prefix, arm, CONCEPTSET_DIR) for arm in AI_ARM_IDS}\n",
    "    \n",
    "    all_files_found = human_file is not None and all(ai_files.values())\n",
    "    if not all_files_found:\n",
    "        print(f\"\\n‚ö†Ô∏è Skipping {task_name} (prefix {disease_prefix}):\")\n",
    "        if not human_file: \n",
    "            print(f\"  - Missing file for {HUMAN_ARM_ID}\")\n",
    "        for arm, f in ai_files.items():\n",
    "            if f is None:\n",
    "                print(f\"  - Missing file for {arm}\")\n",
    "        continue\n",
    "        \n",
    "    # Load concept sets\n",
    "    gs_concepts = load_concepts(gs_file)\n",
    "    human_concepts = load_concepts(human_file)\n",
    "    ai_concepts = {arm: load_concepts(f) for arm, f in ai_files.items()}\n",
    "    \n",
    "    # Define the 'universe' of items for this task\n",
    "    universe = gs_concepts | human_concepts\n",
    "    for arm_concepts in ai_concepts.values():\n",
    "        universe.update(arm_concepts)\n",
    "    \n",
    "    if not universe:\n",
    "        print(f\"Skipping {task_name}: No concepts found.\")\n",
    "        continue\n",
    "\n",
    "    # Create item-level records\n",
    "    for concept_id in universe:\n",
    "        record = {\n",
    "            'task_id': task_name,\n",
    "            'concept_id': concept_id,\n",
    "            'gold': concept_id in gs_concepts,\n",
    "            'human': concept_id in human_concepts,\n",
    "            'weight': weight_lookup.get(concept_id, 0.0) # Get weight, default to 0.0 (log(1+0))\n",
    "        }\n",
    "        # Add AI classifications\n",
    "        for arm in AI_ARM_IDS:\n",
    "            record[arm.lower()] = concept_id in ai_concepts[arm]\n",
    "        \n",
    "        all_items.append(record)\n",
    "    \n",
    "    print(f\"Processed {task_name}: {len(universe)} items.\")\n",
    "    all_tasks_found.append(task_name)\n",
    "\n",
    "# Create the final DataFrame\n",
    "df_items = pd.DataFrame(all_items)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Master dataset created.\")\n",
    "print(f\"Total tasks included: {len(all_tasks_found)}\")\n",
    "print(f\"Total items (concepts): {len(df_items)}\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# ‚ö†Ô∏è BUG FIX: Check if df_items is empty before proceeding\n",
    "# ----------------------------------------------------------------\n",
    "if df_items.empty:\n",
    "    print(\"\\n‚ùå CRITICAL ERROR: No data was loaded into the master DataFrame.\")\n",
    "    print(\"This happens if no tasks were processed (e.g., missing Gold Standard or arm files).\")\n",
    "    print(\"Please check the log output above. Halting execution.\")\n",
    "    # This will cause a NameError if you try to run subsequent cells, which is intended.\n",
    "    raise SystemExit(\"Stopping notebook due to empty master DataFrame.\")\n",
    "else:\n",
    "    display(df_items.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Weighted F1-Score Calculation\n",
    "\n",
    "This function takes a DataFrame and computes the aggregate **Weighted F1-score** for Human and all AI arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_f1_scores(df, classifier_cols):\n",
    "    \"\"\"Calculates aggregate Weighted F1 scores for all specified classifiers.\"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    # Get the gold and weight columns ONCE, as they are constant for all classifiers\n",
    "    is_gold = df['gold']\n",
    "    weights = df['weight']\n",
    "    \n",
    "    for clf in classifier_cols:\n",
    "        # Create boolean mask for this classifier\n",
    "        is_clf = df[clf]\n",
    "        \n",
    "        # Multiply the boolean mask by the weight, then sum\n",
    "        wtp = (is_gold & is_clf) * weights\n",
    "        wfp = (~is_gold & is_clf) * weights\n",
    "        wfn = (is_gold & ~is_clf) * weights\n",
    "        \n",
    "        # Sum the weights\n",
    "        WTP = wtp.sum()\n",
    "        WFP = wfp.sum()\n",
    "        WFN = wfn.sum()\n",
    "        \n",
    "        denominator = (2 * WTP + WFP + WFN)\n",
    "        f1 = (2 * WTP) / denominator if denominator > 0 else 0\n",
    "        scores[clf] = f1\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Run Stratified Bootstrap\n",
    "\n",
    "This is the core of the test. We loop 10,000 times. In each loop, we:\n",
    "1.  Create a new dataset by sampling *with replacement* from each task.\n",
    "2.  Calculate the aggregate **Weighted F1 scores** for this new dataset.\n",
    "3.  Store the difference for each AI: $W-F1_{AI_n} - W-F1_{Human}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original, observed Weighted F1 scores and differences\n",
    "obs_scores = calculate_weighted_f1_scores(df_items, classifier_columns)\n",
    "obs_diffs = {}\n",
    "\n",
    "print(\"--- Observed Weighted F1 Scores ---\")\n",
    "print(f\"Observed W-F1 ({HUMAN_ARM_ID}): {obs_scores['human']:.4f}\")\n",
    "for arm in AI_ARM_IDS:\n",
    "    arm_key = arm.lower()\n",
    "    obs_diff = obs_scores[arm_key] - obs_scores['human']\n",
    "    obs_diffs[arm] = obs_diff\n",
    "    print(f\"Observed W-F1 ({arm}):     {obs_scores[arm_key]:.4f} (Diff: {obs_diff:+.4f})\")\n",
    "\n",
    "# Initialize storage for bootstrap differences\n",
    "bootstrap_diffs = {arm: [] for arm in AI_ARM_IDS}\n",
    "\n",
    "tasks = df_items['task_id'].unique()\n",
    "task_indices = {task: df_items[df_items['task_id'] == task].index for task in tasks}\n",
    "\n",
    "print(f\"\\nRunning {BOOTSTRAP_ITERATIONS} bootstrap iterations...\")\n",
    "\n",
    "# Set a fixed random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "for _ in tqdm(range(BOOTSTRAP_ITERATIONS)):\n",
    "    # Create stratified sample\n",
    "    resampled_indices = []\n",
    "    for task in tasks:\n",
    "        indices = task_indices[task]\n",
    "        # Sample with replacement from this task's items\n",
    "        sample = np.random.choice(indices, size=len(indices), replace=True)\n",
    "        resampled_indices.append(sample)\n",
    "    \n",
    "    # Combine indices and create the bootstrap dataset\n",
    "    df_boot = df_items.loc[np.concatenate(resampled_indices)]\n",
    "    \n",
    "    # Calculate Weighted F1 scores for this sample\n",
    "    scores = calculate_weighted_f1_scores(df_boot, classifier_columns)\n",
    "    f1_h = scores['human']\n",
    "    \n",
    "    # Store the difference for each AI arm\n",
    "    for arm in AI_ARM_IDS:\n",
    "        f1_ai = scores[arm.lower()]\n",
    "        bootstrap_diffs[arm].append(f1_ai - f1_h)\n",
    "\n",
    "print(\"Bootstrap complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Results and Non-Inferiority Conclusion\n",
    "\n",
    "We now analyze the 10,000 Weighted F1 differences for *each AI arm* to build their confidence intervals and make a conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"    FINAL NON-INFERIORITY TEST RESULTS (WEIGHTED F1)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for arm in AI_ARM_IDS:\n",
    "    arm_key = arm.lower()\n",
    "    diffs_arr = np.array(bootstrap_diffs[arm])\n",
    "    \n",
    "    # For a one-sided test at ALPHA=0.05, we need the 5th percentile.\n",
    "    # This corresponds to the lower bound of a 90% two-sided CI.\n",
    "    ci_lower = np.percentile(diffs_arr, 100 * ALPHA)\n",
    "    ci_upper = np.percentile(diffs_arr, 100 * (1 - ALPHA))\n",
    "    mean_diff = np.mean(diffs_arr)\n",
    "    \n",
    "    # --- The Non-Inferiority Test ---\n",
    "    # We REJECT the null (H0: AI is inferior) if the lower bound \n",
    "    # of our CI is GREATER than the negative margin.\n",
    "    is_non_inferior = ci_lower > -NON_INFERIORITY_MARGIN\n",
    "    \n",
    "    # --- Final Report for this Arm ---\n",
    "    print(f\"\\n--- [ {arm} vs. {HUMAN_ARM_ID} ] ---\")\n",
    "    print(f\"Non-Inferiority Margin (Œî): {NON_INFERIORITY_MARGIN:.4f}\")\n",
    "    print(f\"Hypothesis (H1):      W-F1({arm}) - W-F1({HUMAN_ARM_ID}) > -{NON_INFERIORITY_MARGIN}\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Observed W-F1 ({HUMAN_ARM_ID}):  {obs_scores['human']:.4f}\")\n",
    "    print(f\"Observed W-F1 ({arm}):     {obs_scores[arm_key]:.4f}\")\n",
    "    print(f\"Observed Difference:  {obs_diffs[arm]:+.4f}\")\n",
    "    print(f\"Mean Bootstrapped Diff: {mean_diff:+.4f}\")\n",
    "    print(f\"90% Confidence Interval: [{ci_lower:+.4f}, {ci_upper:+.4f}]\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Test Condition:       Lower Bound > -Margin\")\n",
    "    print(f\"Result:               {ci_lower:+.4f} > {-NON_INFERIORITY_MARGIN}\")\n",
    "    print(\"\\n   CONCLUSION:\")\n",
    "    if is_non_inferior:\n",
    "        print(f\"    ‚úÖ The {arm} classifier IS non-inferior to the {HUMAN_ARM_ID} classifier.\")\n",
    "        print(\"       (The lower bound of the 90% CI is above the non-inferiority margin.)\")\n",
    "    else:\n",
    "        print(f\"    ‚ùå The {arm} classifier is NOT non-inferior to the {HUMAN_ARM_ID} classifier.\")\n",
    "        print(\"       (The lower bound of the 90% CI is at or below the non-inferiority margin.)\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"    All tests complete.\")\n",
    "print(\"=\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}